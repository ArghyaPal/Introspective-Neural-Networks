{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "training_wasserstein.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/blufzzz/Introspective-Neural-Networks/blob/master/training_wasserstein.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "4bM125Kfxhty",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from torchvision.transforms import Compose, ToTensor, Normalize, Resize\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.cuda import set_device\n",
        "from torchvision.models.resnet import ResNet, BasicBlock\n",
        "from torchvision.datasets import MNIST\n",
        "from tqdm.autonotebook import tqdm\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "import inspect\n",
        "import time\n",
        "import random\n",
        "from torch import nn, optim\n",
        "import torch\n",
        "from tqdm import tnrange, tqdm_notebook"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mrXARkCuxht4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=1, bias=False)\n",
        "\n",
        "\n",
        "def conv1x1(in_planes, out_planes, stride=1):\n",
        "    \"\"\"1x1 convolution\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv1x1(inplanes, planes)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = conv3x3(planes, planes, stride)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = conv1x1(planes, planes * self.expansion)\n",
        "        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, block, layers, num_classes=1000, zero_init_residual=False):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.inplanes = 64\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n",
        "                               bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
        "        #self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(256 * block.expansion, num_classes)\n",
        "        \n",
        "        self.X = torch.nn.Parameter(torch.empty((10, 1, 28, 28)).normal_(mean=0, std=0.3));\n",
        "        self.X.requires_grad = False;\n",
        "        \n",
        "        self.wass_fc = torch.nn.Linear(10, 1, bias=True)\n",
        "        \n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        # Zero-initialize the last BN in each residual branch,\n",
        "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
        "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
        "        if zero_init_residual:\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, Bottleneck):\n",
        "                    nn.init.constant_(m.bn3.weight, 0)\n",
        "                elif isinstance(m, BasicBlock):\n",
        "                    nn.init.constant_(m.bn2.weight, 0)\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
        "                nn.BatchNorm2d(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        #x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "    \n",
        "    def to_synth(self):\n",
        "        return self.forward(self.X);\n",
        "\n",
        "\n",
        "def resnet18(pretrained=False, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-18 model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs);\n",
        "    if pretrained:\n",
        "        model.load_state_dict(model_zoo.load_url(model_urls['resnet18']))\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "MnistResNet = resnet18();\n",
        "MnistResNet.conv1 = torch.nn.Conv2d(1, 64, kernel_size=(5, 5), stride=(2, 2), padding=(3, 3), bias=False);\n",
        "MnistResNet.fc = torch.nn.Linear(256, 10, bias=True);\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "285wSeeNxht-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class MnistDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root_dir='data/', training=True, train_samples='all'):\n",
        "        self.real_imgs = [];\n",
        "        self.fake_imgs = [];\n",
        "        self.real_labels = [];\n",
        "        self.fake_labels = [];\n",
        "        self.sample_real = True;\n",
        "        self.sampled_fakes = 0;\n",
        "        \n",
        "        self.fake_samples={}\n",
        "        for i in range(10):\n",
        "            self.fake_samples[-i] = [];\n",
        "        #self.fake = [];\n",
        "        \n",
        "        #mnist = MNIST(download=False, train=True, root=\".\").train_data.float()\n",
        "        \n",
        "        if training:\n",
        "            x, y = torch.load(root_dir + 'processed/training.pt');\n",
        "        else:\n",
        "            x, y = torch.load(root_dir + 'processed/test.pt');\n",
        "        if train_samples == 'all':\n",
        "            for i in range(y.shape[0]):\n",
        "                self.real_imgs.append(x[i, ...].float()/255);\n",
        "                self.real_labels.append(y[i].long());\n",
        "        else:\n",
        "            for i in range(train_samples):\n",
        "                self.real_imgs.append(x[i, ...].float()/255);\n",
        "                self.real_labels.append(y[i].long());\n",
        "    \n",
        "    def __len__(self):\n",
        "        return 2*len(self.real_labels);\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.sample_real or len(self.fake_samples[0]) == 0:\n",
        "            x = self.real_imgs[idx%len(self.real_labels)];\n",
        "            y = self.real_labels[idx%len(self.real_labels)];\n",
        "            fake = torch.ByteTensor([0])[0];\n",
        "            self.sample_real = False;\n",
        "            self.y_real=y\n",
        "        else:\n",
        "            y = -self.y_real;\n",
        "            x = random.choice(self.fake_samples[int(y)]);\n",
        "            fake = torch.ByteTensor([1])[0];\n",
        "            self.sample_real = True;\n",
        "        if len(self.fake_samples[0]) == 0:\n",
        "            self.sample_real = True;\n",
        "        return (x.unsqueeze(0)-0.5)/(0.5)*0.6, y.float().unsqueeze(0), fake;\n",
        "    \n",
        "    def add_artificial(self, X):\n",
        "        for i in range(X.shape[0]):\n",
        "            self.fake_samples[-i].append(X[i, 0, ...].detach().cpu());\n",
        "            \n",
        "        pass;\n",
        "\n",
        "\n",
        "def get_data_loaders(train_batch_size, val_batch_size, train_size):\n",
        "    mnist = MNIST(download=False, train=True, root=\".\").train_data.float()\n",
        "\n",
        "    train_loader = DataLoader(MnistDataset(root_dir='', train_samples=train_size),\n",
        "                              batch_size=train_batch_size, shuffle=False)\n",
        "\n",
        "    val_loader = DataLoader(MnistDataset(root_dir='', training=False),\n",
        "                            batch_size=val_batch_size, shuffle=False)\n",
        "    return train_loader, val_loader\n",
        "\n",
        "\n",
        "    \n",
        "def calculate_metric(metric_fn, true_y, pred_y):\n",
        "    if \"average\" in inspect.getfullargspec(metric_fn).args:\n",
        "        return metric_fn(true_y, pred_y, average=\"macro\")\n",
        "    else:\n",
        "        return metric_fn(true_y, pred_y)\n",
        "    \n",
        "    \n",
        "    \n",
        "def print_scores(p, r, f1, a, batch_size):\n",
        "    for name, scores in zip((\"precision\", \"recall\", \"F1\", \"accuracy\"), (p, r, f1, a)):\n",
        "        print(f\"\\t{name.rjust(14, ' ')}: {sum(scores)/batch_size:.4f}\")   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "A7buLt5_HIm5"
      },
      "cell_type": "markdown",
      "source": [
        "# Wasserstein loss"
      ]
    },
    {
      "metadata": {
        "id": "T_2LT0SMxhuF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def wasserstein_loss(model, outputs, X, fakes, scale = 10):\n",
        "        X_wass = model.wass_fc(outputs);\n",
        "        X_1 = X_wass[1-fakes]\n",
        "        X_2 = X_wass[fakes]\n",
        "        \n",
        "        d_real_loss = torch.mean(X_1)\n",
        "        d_fake_loss = torch.mean(X_2)\n",
        "        d_neg_wass_dist = d_fake_loss - d_real_loss;\n",
        "        eps = X.new_empty(X.shape[0]//2, 1, 1, 1).uniform_();\n",
        "        d_inter_images = eps*X[1-fakes] + (1-eps)*X[fakes];\n",
        "        d_inter_images.requires_grad = True;\n",
        "        \n",
        "        d_inter_logits = model(d_inter_images);\n",
        "        d_inter_wass = model.wass_fc(d_inter_logits);\n",
        "        \n",
        "        \n",
        "        d_inter_grad = torch.autograd.grad(d_inter_wass, d_inter_images, grad_outputs=torch.ones_like(d_inter_wass), create_graph=True, only_inputs=True)[0]\n",
        "        \n",
        "        d_inter_grad_norm = torch.norm(d_inter_grad.view(d_inter_grad.shape[0],-1), dim=-1);\n",
        "        d_inter_grad_penalty = torch.mean((d_inter_grad_norm - 1)**2);\n",
        "        \n",
        "        d_wass_loss = d_neg_wass_dist + scale*d_inter_grad_penalty;\n",
        "        \n",
        "        return d_wass_loss;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oXhIYoilxhuM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Synthesis():\n",
        "    def __init__(self, init_std=0.3):\n",
        "        self.init_std = init_std;\n",
        "        \n",
        "    def sample(self, module, num_iter=10, learning_rate=0.01, add_noise=True):\n",
        "        assert isinstance(module.X, torch.nn.Parameter), 'Expected X to be an instance of torch.nn.Parameter';\n",
        "        \n",
        "        module.train(False);\n",
        "        \n",
        "        # we do not want to create a graph and do backprop on net parameters, since we need only gradient of X\n",
        "        for name, param in module.named_parameters():\n",
        "            if name != 'X':\n",
        "                param.requires_grad = False;\n",
        "            else:\n",
        "                param.requires_grad = True;\n",
        "        \n",
        "        module.X.data = module.X.data.normal_(mean=0, std=self.init_std);\n",
        "        \n",
        "        #module.X.data[0, ...] = x[1, ...];\n",
        "        opt = torch.optim.Adam([module.X], lr=learning_rate, amsgrad=True, betas=(0.9, 0.9));\n",
        "        #opt = torch.optim.ASGD(module.parameters(), lr=learning_rate);\n",
        "        std_noise = learning_rate;\n",
        "        loss_function = nn.CrossEntropyLoss()\n",
        "        for i in range(num_iter):\n",
        "            opt.zero_grad();\n",
        "            classes = -torch.sum(torch.diag(module.to_synth()));\n",
        "            classes.backward();\n",
        "                \n",
        "            if add_noise:\n",
        "                module.X.grad += torch.empty_like(module.X.data).normal_(mean=0, std=2*opt.param_groups[0]['lr']);\n",
        "                opt.param_groups[0]['lr'] *= 0.92;\n",
        "            \n",
        "            #module.X.grad = 0.1*torch.sign(module.X.grad);\n",
        "            opt.step()\n",
        "            \n",
        "            for j in range(10):\n",
        "                a = module.X.data[j, ...].min();\n",
        "                b = module.X.data[j, ...].max();\n",
        "                module.X.data[j, ...] -= a;\n",
        "                module.X.data[j, ...] /= (b-a);\n",
        "                module.X.data[j, ...] *= 1.2;\n",
        "                module.X.data[j, ...] -= 0.6;\n",
        "            \n",
        "        module.train(True);\n",
        "        \n",
        "        for name, param in module.named_parameters():\n",
        "            if name != 'X':\n",
        "                param.requires_grad = True;\n",
        "            else:\n",
        "                param.requires_grad = False;\n",
        "        \n",
        "        return module.X.data;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SRXrzLzVxhuT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train(model, train_loader, val_loader, epochs=1, alpha=0.9, lrate=1e-3):\n",
        "\n",
        "    start_ts = time.time()\n",
        "\n",
        "    losses = []\n",
        "    loss_function = nn.CrossEntropyLoss()\n",
        "    nll_loss_function = nn.NLLLoss()\n",
        "    #optimizer = optim.Adam(model.parameters(), amsgrad=True, lr=lrate, betas=(0.0, 0.9))\n",
        "    optimizer = optim.Adam(model.parameters(), amsgrad=True, lr=lrate, betas=(0.0, 0.9));\n",
        "\n",
        "    batches = len(train_loader)\n",
        "    val_batches = len(val_loader)\n",
        "\n",
        "    # training loop + eval loop\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        progress = tqdm(enumerate(train_loader), desc=\"Loss: \", total=batches)\n",
        "        model.train()\n",
        "\n",
        "        for i, data in progress:\n",
        "            X, y, fakes = data[0].cuda(), data[1].cuda().squeeze(-1).long(), data[2].cuda()\n",
        "            \n",
        "            model.zero_grad()\n",
        "            outputs = model(X)\n",
        "            loss = loss_function(outputs[1-fakes], y[1-fakes]);\n",
        "            \n",
        "            if not torch.all(fakes == 0):\n",
        "                loss += 0.01*wasserstein_loss(model,outputs, X, fakes)\n",
        "                \n",
        "                \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            progress.set_description(\"Loss: {:.4f}\".format(loss.item()))\n",
        "\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        val_losses = 0\n",
        "        precision, recall, f1, accuracy = [], [], [], []\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i, data in enumerate(val_loader):\n",
        "            X, y = data[0].cuda(), data[1].cuda().squeeze(-1)\n",
        "            outputs = torch.sigmoid(model(X))\n",
        "            val_losses += loss_function(outputs, y.long())\n",
        "\n",
        "            predicted_classes = torch.max(outputs, 1)[1]\n",
        "\n",
        "            for acc, metric in zip((precision, recall, f1, accuracy), \n",
        "                                   (precision_score, recall_score, f1_score, accuracy_score)):\n",
        "                acc.append(\n",
        "                    calculate_metric(metric, y.cpu(), predicted_classes.cpu())\n",
        "                )\n",
        "\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, training loss: {total_loss/batches}, validation loss: {val_losses/val_batches}\")\n",
        "    print_scores(precision, recall, f1, accuracy, val_batches)\n",
        "    losses.append(total_loss/batches)\n",
        "    pass\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "t7ZkJadmxhuY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "l = 200;\n",
        "tmax = 200;\n",
        "model = MnistResNet.cuda();\n",
        "train_loader, val_loader = get_data_loaders(100, 256, 25000);\n",
        "for t in range(tmax):\n",
        "    print('t = ', t);\n",
        "    #model = MnistResNet.cuda();\n",
        "    model.train(True)\n",
        "    train(model, train_loader, val_loader, epochs=1, alpha=0.5, lrate=0.001);\n",
        "    \n",
        "    model.train(False)\n",
        "    s = Synthesis(init_std=0.3)\n",
        "    for i in range(l):\n",
        "        aug = s.sample(model, num_iter=100, learning_rate=0.1, add_noise=False);\n",
        "        train_loader.dataset.add_artificial(aug);\n",
        "    \n",
        "    plt.figure();\n",
        "    fig, ax = plt.subplots(nrows=1, ncols=10, figsize=(15,3));\n",
        "    for i in range(10):\n",
        "        ax[i].imshow(aug[i,0,...].cpu());\n",
        "        ax[i].axis('off');\n",
        "    plt.show();"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UFybjzbTxhuf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "fig,ax = plt.subplots(nrows=5, ncols=10, figsize=(17,10));\n",
        "for j in range(5):\n",
        "    s = Synthesis(init_std=0.3)\n",
        "    aug = s.sample(model, num_iter=50, learning_rate=1e-1, add_noise=True);\n",
        "    for i in range(aug.shape[0]):\n",
        "            aug[i,...] -= aug[i, ...].min();\n",
        "            aug[i, ...] /= aug[i, ...].max();\n",
        "    for i in range(10):\n",
        "        ax[j][i].imshow(aug[i,0,...].cpu());\n",
        "        ax[j][i].axis('off');"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}